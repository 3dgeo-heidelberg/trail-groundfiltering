{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d925a82d-fe3c-4658-a2fe-e5cd1d6ca772",
   "metadata": {},
   "source": [
    "# Study case 1: Punta Križa, HR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867401b-9856-448e-a995-678f1f341326",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "This LiDAR dataset covers an area at the south coast of Cres Island in Croatia, to the west of Punta Križa. It has a point density of approximately 9 pts/m$^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205a1bf-a49f-4fc8-9de9-00fd4370e963",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Import `afwizard`, set parameters and load your dataset\n",
    "\n",
    "First, we import the `afwizard` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b8951-5a82-4ac1-87fe-d02b3021eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import afwizard as af"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d48b74-5b84-4322-b639-2eb1bb28f625",
   "metadata": {},
   "source": [
    "Now, we have to set the LAStools directory to the path where we installed LAStools. This may be a different location on your computer, so you will have to provide the correct path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb248c-3743-4e1d-9638-5a2e81c99fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "af.set_lastools_directory(\"C:/LAStools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b625896-e90d-44c4-83ce-a70635a42a95",
   "metadata": {},
   "source": [
    "The next steps are to create the directory \"filters\", where we will later save our developed filter pipelines. After that, we set the directory where the input data is located and where the output data will be written. In case, the directory is not existing, it will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715eb7cd-58b6-46a6-bd4e-bf84fd13adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "af.set_data_directory(\"filters\", create_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceabcac-dda0-431c-a508-4c4ed7a59c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "af.set_data_directory(\"data\", create_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb07988-b004-4e35-bbbb-ecdc253722ad",
   "metadata": {},
   "source": [
    "Now we can load our first dataset by file name and additionally provide the spatial reference system. Make sure that the data are copied to your data directory specified just before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c204cb-a638-44c9-9a9a-6306f7fad2aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = af.DataSet(filename=\"PK_last.laz\", spatial_reference=\"EPSG:25833\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e89a874-812a-44d0-a3e2-73c8b089a529",
   "metadata": {},
   "source": [
    "We remove any existing classification from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102fcab-fa2e-41e3-bde8-660707fb0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = af.remove_classification(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76be66-7b75-463d-9897-fbc82a064e7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Restricting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fcd787-f219-4168-8840-baf3107efe89",
   "metadata": {},
   "source": [
    "Since the dataset covers both land and water, the goal is to split our dataset in two segments to process these areas separately. To help us, we load a GeoJSON file containing a pre-prepared segmentation of the LiDAR tile. In case you want to produce your own segmentation, you can use e.g. QGIS: create a shapefile, draw the segments (the shapes must not overlap and there must not be areas in your dataset not covered by a segment) and export the Shapefile as GeoJSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383fa3f-6250-4285-a105-53808979bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = af.load_segmentation(\n",
    "    \"PK_segments.geojson\", spatial_reference=\"EPSG:25833\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6d93c-c5d8-4e8f-b814-38fef19fa590",
   "metadata": {},
   "source": [
    "Since the segments in the segmentation may cover large areas and contain many points, we sample our segments using the `restrict` method of `afwizard`. This will make the process of creating filter pipelines much faster! \n",
    "The aim is to select characteristic subareas, one for each segment running following command. The command will open a GUI showing the segments on a map. If you want to see a draft hillshade version of the DTM, click on \"Visualize\". \n",
    "\n",
    "Click the small polygon on the sidebar on the left to draw a segmentation polygon for the land area. When you are done, finish by clicking \"Finalize\". Your restricted dataset is then saved to the variable `rds`. Save the data by executing the next code cell (`saved = rds.save(...)`). Then repeat the `restrict` and the `save` step for the water area as well. For the water area, the filename is changed to `\"data/sample_uw.las\"` when saving (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b985d8-e6b7-41ba-ba4f-13413a75d068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rds = ds.restrict(segmentation_overlay=segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9029eb-95aa-42e6-b17f-7c85ddf6b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = rds.save(\"data/sample_stonewall.las\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd9750-cce3-4b69-bfbb-ae2cbdd60cb5",
   "metadata": {},
   "source": [
    "The same procedure is now repeated for the submerged terrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292a390-8988-4df1-8058-ecf3e8d64241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rds = ds.restrict(segmentation_overlay=segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b716582-b1f2-4936-b806-4604722a8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = rds.save(\"data/sample_uw.las\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b55610-e184-4cc8-b663-3e7817dd0533",
   "metadata": {},
   "source": [
    "## 3. Creating filter pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd08a6-2906-4663-9c81-f1558621ad13",
   "metadata": {},
   "source": [
    "Now we can create targeted filter pipelines for the two datasets we created before. This is done using the interactive `pipeline_tuning` function. Here, you can select from different outlier filtering and ground filtering algorithms and combine them in a sequential filter pipeline. You can preview the result of different versions of your pipelines and delete filtering steps if it turns out they are not needed or need different settings. We do this step twice, for each of our two segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe6241-f04b-48a8-878d-90e71be37a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledataset1 = af.DataSet(filename=\"data/sample_stonewall.las\", spatial_reference=\"EPSG:25833\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80014de8-cb22-4f42-9991-b013e3ca8374",
   "metadata": {},
   "source": [
    "The following command will open the GUI for creating a filter pipeline. In the centre, the point cloud of the spatial sample is visualized as hillshade, slope or a combination of both. Visualization settings can be tuned using the fields in the right column. \n",
    "\n",
    "In the center, a tab (#0) with an image is displayed showing the unclassified data as hillshade.\n",
    "\n",
    "In the left column, a filter can be added to the pipeline. Click on \"Pipeline stages\" and then on \"+ Add entry\" and select the filter algorithm of your choice (here: lasground_new (LASTools)). Finally, you can set the parameters (offset, spike, step). Here, we let them at their defaut values. (0, 0.5, 1). Clicking \"Preview\" button on top of the right column will trigger the filtering of the sample using the current filter and parameter strings and display the result as a new window in tab \"#1\" in the centre column (with the preset hillshade at 45° altitude and 0.2 m resolution). Before finalizing a finished filtering pipeline, fill in the \"Pipeline metadata\" in the dropdown on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e61b2a-1bac-473f-95f3-3991f2ec2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = af.pipeline_tuning(sampledataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77411e17-d5b1-419b-a1da-21f4cea2b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "af.save_filter(pipeline1, \"filters/PK_stonewall.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf1f1d-dcda-448d-bbec-ff5d12523e17",
   "metadata": {},
   "source": [
    "Now we repeat the procedure for the other segment using our sample from the terrestrial area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006b172-7984-4497-a6fc-7a4858107986",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledataset2 = af.DataSet(filename=\"sample_uw.las\", spatial_reference=\"EPSG:25833\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd90bf-c6a0-4172-97f0-644b8c349973",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = af.pipeline_tuning(sampledataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d2e65-750f-490d-b587-056f4e0317c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "af.save_filter(pipeline2, \"filters/PK_uw.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3b6a7-f440-4d59-9fd6-62b21da3f0fd",
   "metadata": {},
   "source": [
    "## 4. Mapping segmentations to filter pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8735dbb-2207-40d2-849f-bd17c5ac74d9",
   "metadata": {},
   "source": [
    "Now we map the created filter pipelines to the segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44778abd-069e-4859-bbd1-cc3f788357d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "af.add_filter_library(path=\"filters\", recursive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b9f7d-ccbf-4cbc-bb03-f3ac88ee2a0e",
   "metadata": {},
   "source": [
    "Using the selection window below, select the two pipelines you created earlier.\n",
    "**Important**: You need to select all pipelines that you will need to attach to the segment classes in the following step. Select multiple pipelines by pressing \"Ctrl\" (\"Strg\" on German keyboards). As before, finish the interactive step by clicking \"Finalize\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9508b-c9c4-42a9-97ab-4db666fea0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = af.select_pipeline_from_library(\"filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ace0a-201e-4891-9ef3-a264e60a053b",
   "metadata": {},
   "source": [
    "In this stage, you select which pipeline to assign to which segment. On the top right, switch in the dropdown menu to \"segment\". You will see that there are now two segments, \"UW\" for \"underwater\" and \"veg\" for \"vegetation\". You can also visualize their outline by clicking on the marker button next to the segment name. In the dropdown menu below each segment, select the corresponding filter you created. When you are done with assigning, click \"Finalize\". Then run the next code cell to save the segments together with their assigned filter pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e976ba-9142-43c2-a2d6-b6e402efd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_segmentation = af.assign_pipeline(\n",
    "    ds, segmentation=segmentation, pipelines=pipelines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae084e-b921-4313-8ae6-2d4638e5383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_segmentation.save(\"data/PK_segments_assigned.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db647b6-8dba-4323-86bb-4b218cdbf097",
   "metadata": {},
   "source": [
    "## 5. Adaptive filtering by running AFwizard  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b30cc3-73bb-4155-b09f-97b24b55b46d",
   "metadata": {},
   "source": [
    "Finally, the adaptive filtering has to be calculated. This is done on a command line basis. Therefore, open a new Tab using the \"+\" symbol next to the top of this window and cklick on \"Terminal\". \n",
    "ATTENTION: the directories refer relatively to the directory of the prompt in the terminal!!\n",
    "\n",
    "In the example below, the prompt is executed from `.../TRAIL_groundfiltering/docs`:\n",
    "Filesystem is: \n",
    "`.../TRAIL_groundfiltering/docs/data`\n",
    "`.../TRAIL_groundfiltering/docs/filters`  \n",
    "\n",
    "- filename PK_last.laz is in directory \"data\"\n",
    "- filename PK_segments_assigned.geojson is in directory \"data\"\n",
    "- Directory \"output\" will be created in data/output\n",
    "- filters are stored in \"filters\"\n",
    "\n",
    "```bash\n",
    "afwizard --dataset=data/PK_last.laz --dataset-crs=EPSG:25833 --segmentation=data/PK_segments_assigned.geojson --segmentation-crs=EPSG:25833 --output-dir=data/output --library filters --lastools-dir=\"C:\\LAStools\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0f185-c760-48ef-b970-a093e9249da7",
   "metadata": {},
   "source": [
    "We can also execute the command line tool by using the `!command` syntax in the Jupyter notebook. The exclamation mark tells Jupyter that it should pass the command to the shell instead of executing with Python. Let's try it out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac51a02-1f78-4bed-8cfa-16f79bd226b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!afwizard --dataset=data/PK_last.laz --dataset-crs=EPSG:25833 --segmentation=data/PK_segments_assigned.geojson --segmentation-crs=EPSG:25833 --output-dir=data/output --library filters --lastools-dir=\"C:/LAStools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10591ea1-30b4-4f70-acb7-278715ca2086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
